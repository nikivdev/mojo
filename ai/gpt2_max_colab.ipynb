{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 with MAX on GPU\n",
    "\n",
    "Run GPT-2 inference using Modular's MAX framework.\n",
    "\n",
    "**Setup:** Runtime → Change runtime type → T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q numpy torch transformers rich\n",
    "!pip install -q max mojo --index-url https://dl.modular.com/public/nightly/python/simple/ --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "from max.driver import CPU, GPU\n",
    "from max.dtype import DType\n",
    "from max.experimental import functional as F\n",
    "from max.experimental.tensor import Tensor, TensorType\n",
    "from max.graph import DeviceRef\n",
    "from max.nn.module_v3 import Embedding, Linear, Module, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Config\n",
    "class GPT2Config:\n",
    "    vocab_size = 50257\n",
    "    n_positions = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    layer_norm_epsilon = 1e-5\n",
    "\n",
    "print(f\"Config: {GPT2Config.n_layer} layers, {GPT2Config.n_head} heads, {GPT2Config.n_embd} dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization\n",
    "class LayerNorm(Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.weight = Tensor.ones([dim])\n",
    "        self.bias = Tensor.zeros([dim])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return F.layer_norm(x, gamma=self.weight, beta=self.bias, epsilon=self.eps)\n",
    "\n",
    "\n",
    "# Causal Mask\n",
    "@F.functional\n",
    "def causal_mask(seq_len, dtype, device):\n",
    "    from max.graph import Dim\n",
    "    n = Dim(seq_len)\n",
    "    mask = Tensor.constant(float(\"-inf\"), dtype=dtype, device=device)\n",
    "    mask = F.broadcast_to(mask, shape=(seq_len, n))\n",
    "    return F.band_part(mask, num_lower=None, num_upper=0, exclude=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head Attention\n",
    "class GPT2Attention(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        self.c_attn = Linear(self.n_embd, 3 * self.n_embd, bias=True)\n",
    "        self.c_proj = Linear(self.n_embd, self.n_embd, bias=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = F.split(qkv, [self.n_embd, self.n_embd, self.n_embd], axis=2)\n",
    "        \n",
    "        q = q.reshape([B, T, self.n_head, self.head_dim]).transpose(-3, -2)\n",
    "        k = k.reshape([B, T, self.n_head, self.head_dim]).transpose(-3, -2)\n",
    "        v = v.reshape([B, T, self.n_head, self.head_dim]).transpose(-3, -2)\n",
    "        \n",
    "        scale = self.head_dim ** -0.5\n",
    "        attn = (q @ k.transpose(-1, -2)) * scale\n",
    "        mask = causal_mask(T, 0, dtype=attn.dtype, device=attn.device)\n",
    "        attn = attn + mask\n",
    "        attn = F.softmax(attn)\n",
    "        out = attn @ v\n",
    "        \n",
    "        out = out.transpose(-3, -2).reshape([B, T, C])\n",
    "        return self.c_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class GPT2MLP(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = Linear(config.n_embd, 4 * config.n_embd, bias=True)\n",
    "        self.c_proj = Linear(4 * config.n_embd, config.n_embd, bias=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.gelu(x, approximate=\"tanh\")\n",
    "        return self.c_proj(x)\n",
    "\n",
    "\n",
    "# Transformer Block\n",
    "class GPT2Block(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full GPT-2 Model\n",
    "class GPT2(Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.wte = Embedding(config.vocab_size, dim=config.n_embd)\n",
    "        self.wpe = Embedding(config.n_positions, dim=config.n_embd)\n",
    "        self.h = Sequential(*(GPT2Block(config) for _ in range(config.n_layer)))\n",
    "        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.lm_head = Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def __call__(self, input_ids):\n",
    "        B, T = input_ids.shape\n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos = Tensor.arange(T, dtype=input_ids.dtype, device=input_ids.device)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.h(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.lm_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation function\n",
    "def generate(model, tokenizer, device, prompt, max_tokens=30, temperature=0.8):\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    input_ids = Tensor.constant([tokens], dtype=DType.int64, device=device)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i in range(max_tokens):\n",
    "        logits = model(input_ids)\n",
    "        next_logits = logits[0, -1, :]\n",
    "        \n",
    "        if temperature > 0:\n",
    "            next_logits = next_logits / Tensor.constant(temperature, dtype=next_logits.dtype, device=device)\n",
    "            probs = F.softmax(next_logits)\n",
    "            probs_np = np.from_dlpack(probs.to(CPU()))\n",
    "            next_id = np.random.choice(len(probs_np), p=probs_np)\n",
    "        else:\n",
    "            next_id = int(np.from_dlpack(F.argmax(next_logits).to(CPU())))\n",
    "        \n",
    "        next_tensor = Tensor.constant([[next_id]], dtype=DType.int64, device=device)\n",
    "        input_ids = F.concat([input_ids, next_tensor], axis=1)\n",
    "        \n",
    "        if next_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        # Print progress every 10 tokens\n",
    "        if (i + 1) % 10 == 0:\n",
    "            current = tokenizer.decode(np.from_dlpack(input_ids.to(CPU())).flatten().tolist())\n",
    "            print(f\"[{i+1}] {current}\")\n",
    "    \n",
    "    result_ids = np.from_dlpack(input_ids.to(CPU())).flatten().tolist()\n",
    "    return tokenizer.decode(result_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace model and tokenizer\n",
    "print(\"Loading GPT-2 from HuggingFace...\")\n",
    "hf_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MAX model and load weights\n",
    "# Use GPU() for T4, or CPU() for CPU-only\n",
    "try:\n",
    "    device = GPU()\n",
    "    print(f\"Using GPU: {device}\")\n",
    "except:\n",
    "    device = CPU()\n",
    "    print(f\"GPU not available, using CPU: {device}\")\n",
    "\n",
    "config = GPT2Config()\n",
    "model = GPT2(config)\n",
    "\n",
    "print(\"Loading weights...\")\n",
    "model.load_state_dict(hf_model.state_dict())\n",
    "model.to(device)\n",
    "\n",
    "# Transpose Conv1D weights to Linear format\n",
    "for name, child in model.descendents:\n",
    "    if isinstance(child, Linear):\n",
    "        if any(n in name for n in [\"c_attn\", \"c_proj\", \"c_fc\"]):\n",
    "            child.weight = child.weight.T\n",
    "\n",
    "print(\"Weights loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for faster inference\n",
    "print(\"Compiling model (this may take a minute)...\")\n",
    "token_type = TensorType(DType.int64, (\"batch\", \"seq\"), device=DeviceRef.from_device(device))\n",
    "compiled = model.compile(token_type)\n",
    "print(\"Compilation done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text!\n",
    "result = generate(compiled, tokenizer, device, \"The meaning of life is\", max_tokens=50, temperature=0.8)\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prompts!\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The future of AI is\",\n",
    "    \"In a galaxy far away\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    result = generate(compiled, tokenizer, device, prompt, max_tokens=30, temperature=0.7)\n",
    "    print(f\"\\nFinal: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: measure tokens per second\n",
    "import time\n",
    "\n",
    "prompt = \"Hello world\"\n",
    "num_tokens = 50\n",
    "\n",
    "start = time.time()\n",
    "result = generate(compiled, tokenizer, device, prompt, max_tokens=num_tokens, temperature=0.8)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nGenerated {num_tokens} tokens in {elapsed:.2f}s\")\n",
    "print(f\"Speed: {num_tokens / elapsed:.1f} tokens/sec\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
